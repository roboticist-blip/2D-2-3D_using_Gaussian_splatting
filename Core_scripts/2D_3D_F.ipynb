{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgAHMyBHckco"
      },
      "source": [
        "# 3D Gaussian Splatting Training Pipeline\n",
        "### Professional Implementation for Research\n",
        "\n",
        "**Author:** Sumit maheshwari\n",
        "**Date:** February 2026  \n",
        "**Description:** Complete pipeline for training 3D Gaussian Splatting models from video data\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Architecture\n",
        "1. **Data Upload & Validation**\n",
        "2. **COLMAP Structure-from-Motion (SfM)**\n",
        "3. **3D Gaussian Splatting Training**\n",
        "4. **Model Export (PLY format)**\n",
        "5. **Visualization & Quality Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiE3gIJscq17"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "sD8BdURNcbyJ",
        "outputId": "aa31eae3-823e-4a64-c6fd-8c6974f3b9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        },
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3365110648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mount Google Drive for data persistence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Mount Google Drive for data persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISqqLlKQcuC2"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install plyfile tqdm pillow opencv-python\n",
        "\n",
        "# Install COLMAP\n",
        "!apt-get install -y colmap\n",
        "\n",
        "# Clone 3D Gaussian Splatting repository\n",
        "!git clone https://github.com/graphdeco-inria/gaussian-splatting --recursive\n",
        "%cd gaussian-splatting\n",
        "\n",
        "# Install submodules\n",
        "!pip install submodules/diff-gaussian-rasterization\n",
        "!pip install submodules/simple-knn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQIuFiGcx50"
      },
      "source": [
        "## 2. Data Upload & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M5jaOBFcuBu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "# Configuration\n",
        "PROJECT_NAME = \"2D-2-3D\"\n",
        "\n",
        "DATA_ROOT = \"/content/training_data\"   # ROOT\n",
        "OUTPUT_ROOT = f\"/content/drive/MyDrive/gaussian_splatting/{PROJECT_NAME}\"\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        "print(f\"Output root: {OUTPUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhQKY8xKc7jy"
      },
      "outputs": [],
      "source": [
        "# Upload preprocessed data (zip file containing extracted frames)\n",
        "# Option 1: Upload from local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(DATA_ROOT)\n",
        "        print(f\"Extracted {filename} to {DATA_ROOT}\")\n",
        "\n",
        "# Option 2: Copy from Google Drive (uncomment if using)\n",
        "# DRIVE_DATA_PATH = \"/content/drive/MyDrive/your_data.zip\"\n",
        "# shutil.copy(DRIVE_DATA_PATH, DATA_ROOT)\n",
        "\n",
        "# Validate data structure\n",
        "images_dir = os.path.join(DATA_ROOT, \"training_data\", \"images\")\n",
        "\n",
        "assert os.path.isdir(images_dir), \"❌ images directory not found\"\n",
        "\n",
        "num_images = len([\n",
        "    f for f in os.listdir(images_dir)\n",
        "    if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
        "])\n",
        "\n",
        "print(f\"✓ Found {num_images} images in {images_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Ayhv7Ac9tb"
      },
      "source": [
        "## 3. COLMAP Structure-from-Motion\n",
        "\n",
        "COLMAP performs:\n",
        "- Feature extraction\n",
        "- Feature matching\n",
        "- Sparse reconstruction\n",
        "- Camera pose estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqG2kALuc-m6"
      },
      "outputs": [],
      "source": [
        "# COLMAP configuration\n",
        "COLMAP_DB = os.path.join(DATA_ROOT, \"database.db\")\n",
        "SPARSE_DIR = os.path.join(DATA_ROOT, \"sparse\")\n",
        "os.makedirs(SPARSE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Starting COLMAP SfM pipeline...\")\n",
        "print(\"This may take 10-30 minutes depending on the number of images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st0XgdDxdDsR"
      },
      "outputs": [],
      "source": [
        "!QT_QPA_PLATFORM=offscreen DISPLAY= colmap feature_extractor \\\n",
        "    --database_path {COLMAP_DB} \\\n",
        "    --image_path {IMAGES_DIR} \\\n",
        "    --ImageReader.single_camera 1 \\\n",
        "    --ImageReader.camera_model OPENCV \\\n",
        "    --SiftExtraction.use_gpu 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpTx89kodF33"
      },
      "outputs": [],
      "source": [
        "# Feature matching\n",
        "!colmap exhaustive_matcher \\\n",
        "    --database_path {COLMAP_DB} \\\n",
        "    --SiftMatching.use_gpu 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIre7jxldHFe"
      },
      "outputs": [],
      "source": [
        "# Sparse reconstruction (mapper)\n",
        "!colmap mapper \\\n",
        "    --database_path {COLMAP_DB} \\\n",
        "    --image_path {images_dir} \\\n",
        "    --output_path {SPARSE_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFYUCX6tdKfL"
      },
      "outputs": [],
      "source": [
        "# Convert COLMAP binary to text format (for inspection)\n",
        "import os\n",
        "\n",
        "SPARSE_MODEL_DIR = os.path.join(SPARSE_DIR, \"0\")\n",
        "SPARSE_TEXT_DIR = os.path.join(DATA_ROOT, \"sparse_text\")\n",
        "\n",
        "# THIS LINE IS MANDATORY\n",
        "os.makedirs(SPARSE_TEXT_DIR, exist_ok=True)\n",
        "\n",
        "# Safety check (optional but smart)\n",
        "assert os.path.isdir(SPARSE_MODEL_DIR), \"❌ sparse/0 does not exist\"\n",
        "\n",
        "!QT_QPA_PLATFORM=offscreen DISPLAY= colmap model_converter \\\n",
        "    --input_path {SPARSE_MODEL_DIR} \\\n",
        "    --output_path {SPARSE_TEXT_DIR} \\\n",
        "    --output_type TXT\n",
        "\n",
        "# Display reconstruction statistics\n",
        "points3D_txt = os.path.join(SPARSE_TEXT_DIR, \"points3D.txt\")\n",
        "\n",
        "if os.path.isfile(points3D_txt):\n",
        "    with open(points3D_txt, \"r\") as f:\n",
        "        num_points = sum(\n",
        "            1 for line in f\n",
        "            if line.strip() and not line.startswith(\"#\")\n",
        "        )\n",
        "    print(\"\\n✓ COLMAP reconstruction successful!\")\n",
        "    print(f\"  - Reconstructed 3D points: {num_points}\")\n",
        "else:\n",
        "    raise RuntimeError(\"❌ points3D.txt not created — conversion failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0N-NdudNRn"
      },
      "source": [
        "## 4. 3D Gaussian Splatting Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD3gWA16dRXx"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "ITERATIONS = 30000  # Standard: 30k iterations\n",
        "MODEL_PATH = os.path.join(OUTPUT_ROOT, \"output\")\n",
        "\n",
        "# Hyperparameters (can be tuned for your specific use case)\n",
        "POSITION_LR_INIT = 0.00016\n",
        "POSITION_LR_FINAL = 0.0000016\n",
        "FEATURE_LR = 0.0025\n",
        "OPACITY_LR = 0.05\n",
        "SCALING_LR = 0.005\n",
        "ROTATION_LR = 0.001\n",
        "\n",
        "print(f\"Training for {ITERATIONS} iterations\")\n",
        "print(f\"Model will be saved to: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkZ1AfPOdpAn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "%cd /content/gaussian-splatting\n",
        "\n",
        "print(f\"Contents of {COLMAP_GS_INPUT_DIR}/sparse/0/\")\n",
        "!ls -F {COLMAP_GS_INPUT_DIR}/sparse/0/\n",
        "\n",
        "print(f\"Does images.bin exist: {os.path.exists(os.path.join(COLMAP_GS_INPUT_DIR, 'sparse', '0', 'images.bin'))}\")\n",
        "print(f\"Does images.txt exist: {os.path.exists(os.path.join(COLMAP_GS_INPUT_DIR, 'sparse', '0', 'images.txt'))}\")\n",
        "\n",
        "# --- New diagnostic step: Print content of cameras.txt ---\n",
        "cameras_txt_path = os.path.join(COLMAP_GS_INPUT_DIR, 'sparse', '0', 'cameras.txt')\n",
        "if os.path.exists(cameras_txt_path):\n",
        "    print(f\"\\nContent of {cameras_txt_path}:\")\n",
        "    with open(cameras_txt_path, 'r') as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(f\"\\nWARNING: {cameras_txt_path} not found.\")\n",
        "# --------------------------------------------------------\n",
        "\n",
        "!python train.py \\\n",
        "    -s {COLMAP_GS_INPUT_DIR} \\\n",
        "    -m {MODEL_PATH} \\\n",
        "    --iterations {ITERATIONS} \\\n",
        "    --save_iterations 7000 15000 30000 \\\n",
        "    --checkpoint_iterations 7000 15000 30000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSPmzjBHdUZi"
      },
      "source": [
        "## 5. Model Export & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_1g6hFBdVql"
      },
      "outputs": [],
      "source": [
        "# Locate the final PLY file\n",
        "FINAL_PLY = os.path.join(MODEL_PATH, \"point_cloud\", f\"iteration_{ITERATIONS}\", \"point_cloud.ply\")\n",
        "\n",
        "if os.path.exists(FINAL_PLY):\n",
        "    file_size_mb = os.path.getsize(FINAL_PLY) / (1024 * 1024)\n",
        "    print(f\"✓ Model successfully saved: {FINAL_PLY}\")\n",
        "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "    # Copy to Google Drive for persistence\n",
        "    drive_ply = os.path.join(OUTPUT_ROOT, f\"{PROJECT_NAME}_final.ply\")\n",
        "    shutil.copy(FINAL_PLY, drive_ply)\n",
        "    print(f\"✓ Copied to Google Drive: {drive_ply}\")\n",
        "else:\n",
        "    print(\"✗ PLY file not found. Training may have failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or0dUDo_dXh9"
      },
      "outputs": [],
      "source": [
        "# Analyze the PLY file\n",
        "from plyfile import PlyData\n",
        "\n",
        "if os.path.exists(FINAL_PLY):\n",
        "    plydata = PlyData.read(FINAL_PLY)\n",
        "    num_gaussians = len(plydata['vertex'])\n",
        "\n",
        "    print(\"\\n=== Model Statistics ===\")\n",
        "    print(f\"Number of Gaussians: {num_gaussians:,}\")\n",
        "    print(f\"Properties: {plydata['vertex'].data.dtype.names}\")\n",
        "\n",
        "    # Calculate model complexity metrics\n",
        "    import numpy as np\n",
        "    positions = np.stack([plydata['vertex']['x'],\n",
        "                         plydata['vertex']['y'],\n",
        "                         plydata['vertex']['z']], axis=1)\n",
        "\n",
        "    bbox_min = positions.min(axis=0)\n",
        "    bbox_max = positions.max(axis=0)\n",
        "    scene_extent = np.linalg.norm(bbox_max - bbox_min)\n",
        "\n",
        "    print(f\"\\nScene Extent: {scene_extent:.3f} units\")\n",
        "    print(f\"Bounding Box:\")\n",
        "    print(f\"  Min: {bbox_min}\")\n",
        "    print(f\"  Max: {bbox_max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUKSxv7idZh3"
      },
      "source": [
        "## 6. Rendering & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvV4yFSSdcf3"
      },
      "outputs": [],
      "source": [
        "# Render validation views\n",
        "RENDER_OUTPUT = os.path.join(OUTPUT_ROOT, \"renders\")\n",
        "\n",
        "!python render.py \\\n",
        "    -m {MODEL_PATH} \\\n",
        "    --iteration {ITERATIONS} \\\n",
        "    --skip_train \\\n",
        "    --skip_test\n",
        "\n",
        "print(f\"\\nRendered images saved to: {os.path.join(MODEL_PATH, 'test')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWonpqFwdwHo"
      },
      "outputs": [],
      "source": [
        "# Display sample renders\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "render_dir = os.path.join(MODEL_PATH, \"test\", f\"ours_{ITERATIONS}\", \"renders\")\n",
        "render_files = sorted(glob.glob(os.path.join(render_dir, \"*.png\")))[:6]\n",
        "\n",
        "if render_files:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(render_files):\n",
        "        img = Image.open(img_path)\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(f\"View {idx+1}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_ROOT, \"render_preview.png\"), dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Preview saved to: {os.path.join(OUTPUT_ROOT, 'render_preview.png')}\")\n",
        "else:\n",
        "    print(\"No render images found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg8nnr2SdxSn"
      },
      "source": [
        "## 7. Quality Metrics & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjGEGYnsd0TL"
      },
      "outputs": [],
      "source": [
        "# Compute metrics (PSNR, SSIM, LPIPS)\n",
        "!python metrics.py \\\n",
        "    -m {MODEL_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyKA2eIjd2Bk"
      },
      "outputs": [],
      "source": [
        "# Load and display metrics\n",
        "metrics_file = os.path.join(MODEL_PATH, \"results.json\")\n",
        "\n",
        "if os.path.exists(metrics_file):\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    print(\"\\n=== Quality Metrics ===\")\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"\\n{key}:\")\n",
        "            for subkey, subvalue in value.items():\n",
        "                print(f\"  {subkey}: {subvalue:.4f}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "else:\n",
        "    print(\"Metrics file not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rab5C6otd3uU"
      },
      "source": [
        "## 8. Export Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSeWfmQQd3LQ"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive export package\n",
        "import datetime\n",
        "\n",
        "EXPORT_DIR = os.path.join(OUTPUT_ROOT, \"export\")\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy essential files\n",
        "shutil.copy(FINAL_PLY, os.path.join(EXPORT_DIR, \"model.ply\"))\n",
        "\n",
        "if os.path.exists(metrics_file):\n",
        "    shutil.copy(metrics_file, os.path.join(EXPORT_DIR, \"metrics.json\"))\n",
        "\n",
        "# Create experiment report\n",
        "report = {\n",
        "    \"experiment_name\": PROJECT_NAME,\n",
        "    \"date\": datetime.datetime.now().isoformat(),\n",
        "    \"training_iterations\": ITERATIONS,\n",
        "    \"num_input_images\": num_images,\n",
        "    \"num_gaussians\": num_gaussians if 'num_gaussians' in locals() else \"N/A\",\n",
        "    \"model_size_mb\": file_size_mb if 'file_size_mb' in locals() else \"N/A\",\n",
        "    \"hyperparameters\": {\n",
        "        \"position_lr_init\": POSITION_LR_INIT,\n",
        "        \"position_lr_final\": POSITION_LR_FINAL,\n",
        "        \"feature_lr\": FEATURE_LR,\n",
        "        \"opacity_lr\": OPACITY_LR,\n",
        "        \"scaling_lr\": SCALING_LR,\n",
        "        \"rotation_lr\": ROTATION_LR\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(EXPORT_DIR, \"experiment_report.json\"), 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Export package created at: {EXPORT_DIR}\")\n",
        "print(\"\\nContents:\")\n",
        "for item in os.listdir(EXPORT_DIR):\n",
        "    print(f\"  - {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQB5oofZd8G5"
      },
      "outputs": [],
      "source": [
        "# Download final PLY file\n",
        "from google.colab import files\n",
        "\n",
        "final_export_ply = os.path.join(EXPORT_DIR, \"model.ply\")\n",
        "if os.path.exists(final_export_ply):\n",
        "    print(\"Downloading PLY file...\")\n",
        "    files.download(final_export_ply)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(\"PLY file not found for download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovb8a-zEd945"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook provides a complete pipeline for:\n",
        "1. ✓ Data validation and preparation\n",
        "2. ✓ COLMAP Structure-from-Motion\n",
        "3. ✓ 3D Gaussian Splatting training\n",
        "4. ✓ PLY model export\n",
        "5. ✓ Quality metrics evaluation\n",
        "6. ✓ Visualization and rendering\n",
        "\n",
        "**Next Steps for Publication:**\n",
        "- Run ablation studies with different hyperparameters\n",
        "- Compare against baseline methods (NeRF, etc.)\n",
        "- Conduct user studies for perceptual quality\n",
        "- Document computational requirements and runtime\n",
        "- Generate figures and visualizations for paper\n",
        "\n",
        "**Citation:**\n",
        "```\n",
        "@article{kerbl20233d,\n",
        "  title={3D Gaussian Splatting for Real-Time Radiance Field Rendering},\n",
        "  author={Kerbl, Bernhard and Kopanas, Georgios and Leimk{\\\"u}hler, Thomas and Drettakis, George},\n",
        "  journal={ACM Transactions on Graphics},\n",
        "  volume={42},\n",
        "  number={4},\n",
        "  year={2023}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
